Run training...
Experiment dir : /m/triton/scratch/elec/puhe/p/jaina5/transformer-xl/FinnishXL/-Ktrain/20191119-133110
Loading cached dataset...
====================================================================================================
    - data : /m/triton/scratch/elec/puhe/p/jaina5/transformer-xl/FinnishXL/data/kiel_data/
    - dataset : Ktrain
    - n_layer : 72
    - n_head : 8
    - d_head : 40
    - d_embed : 256
    - d_model : 256
    - d_inner : 1024
    - dropout : 0.05
    - dropatt : 0.05
    - init : normal
    - emb_init : normal
    - init_range : 0.1
    - emb_init_range : 0.01
    - init_std : 0.02
    - proj_init_std : 0.01
    - optim : adam
    - lr : 0.00025
    - mom : 0.0
    - scheduler : cosine
    - warmup_step : 0
    - decay_rate : 0.5
    - lr_min : 0.0
    - clip : 0.25
    - clip_nonemb : False
    - max_step : 1200000
    - batch_size : 512
    - batch_chunk : 4
    - tgt_len : 32
    - eval_tgt_len : 32
    - ext_len : 0
    - mem_len : 32
    - not_tied : False
    - seed : 1111
    - cuda : True
    - adaptive : False
    - div_val : 1
    - pre_lnorm : False
    - varlen : False
    - multi_gpu : False
    - log_interval : 200
    - eval_interval : 4000
    - work_dir : /m/triton/scratch/elec/puhe/p/jaina5/transformer-xl/FinnishXL/-Ktrain/20191119-133110
    - restart : True
    - restart_dir : /m/triton/scratch/elec/puhe/p/jaina5/transformer-xl/FinnishXL/-Ktrain/20191112-103726
    - debug : False
    - same_length : False
    - attn_type : 0
    - clamp_len : -1
    - eta_min : 0.0
    - gpu0_bsz : -1
    - max_eval_steps : -1
    - sample_softmax : -1
    - patience : 0
    - finetune_v2 : False
    - finetune_v3 : False
    - fp16 : False
    - static_loss_scale : 1
    - dynamic_loss_scale : False
    - tied : True
    - n_token : 34519
    - n_all_param : 76277847
    - n_nonemb_param : 67405824
====================================================================================================
#params = 76277847
#non emb params = 67405824
| epoch   1 step  1184200 |    200 batches | lr 1.07e-07 | ms/batch 2041.13 | loss  3.68 | ppl    39.538
| epoch   1 step  1184400 |    400 batches | lr 1.05e-07 | ms/batch 2036.05 | loss  3.68 | ppl    39.700
| epoch   1 step  1184600 |    600 batches | lr 1.02e-07 | ms/batch 2038.16 | loss  3.68 | ppl    39.771
| epoch   1 step  1184800 |    800 batches | lr 9.92e-08 | ms/batch 2032.66 | loss  3.68 | ppl    39.608
| epoch   1 step  1185000 |   1000 batches | lr 9.66e-08 | ms/batch 2030.62 | loss  3.68 | ppl    39.805
| epoch   1 step  1185200 |   1200 batches | lr 9.41e-08 | ms/batch 2036.22 | loss  3.68 | ppl    39.586
| epoch   1 step  1185400 |   1400 batches | lr 9.15e-08 | ms/batch 2034.26 | loss  3.68 | ppl    39.756
| epoch   1 step  1185600 |   1600 batches | lr 8.91e-08 | ms/batch 2036.43 | loss  3.69 | ppl    39.880
| epoch   1 step  1185800 |   1800 batches | lr 8.66e-08 | ms/batch 2037.97 | loss  3.69 | ppl    39.888
| epoch   1 step  1186000 |   2000 batches | lr 8.42e-08 | ms/batch 2032.78 | loss  3.68 | ppl    39.839
| epoch   1 step  1186200 |   2200 batches | lr 8.18e-08 | ms/batch 2034.63 | loss  3.68 | ppl    39.647
| epoch   1 step  1186400 |   2400 batches | lr 7.94e-08 | ms/batch 2030.24 | loss  3.69 | ppl    39.867
| epoch   1 step  1186600 |   2600 batches | lr 7.71e-08 | ms/batch 2037.18 | loss  3.69 | ppl    39.988
| epoch   1 step  1186800 |   2800 batches | lr 7.48e-08 | ms/batch 2030.77 | loss  3.69 | ppl    39.852
| epoch   1 step  1187000 |   3000 batches | lr 7.26e-08 | ms/batch 2032.41 | loss  3.68 | ppl    39.692
| epoch   1 step  1187200 |   3200 batches | lr 7.04e-08 | ms/batch 2034.04 | loss  3.69 | ppl    39.913
| epoch   1 step  1187400 |   3400 batches | lr 6.82e-08 | ms/batch 2040.54 | loss  3.69 | ppl    39.987
| epoch   1 step  1187600 |   3600 batches | lr 6.6e-08 | ms/batch 2032.53 | loss  3.68 | ppl    39.687
| epoch   1 step  1187800 |   3800 batches | lr 6.39e-08 | ms/batch 2036.05 | loss  3.68 | ppl    39.806
| epoch   1 step  1188000 |   4000 batches | lr 6.18e-08 | ms/batch 2031.95 | loss  3.68 | ppl    39.721
----------------------------------------------------------------------------------------------------
| Eval 297 at step  1188000 | time: 8692.72s | valid loss  3.72 | valid ppl    41.442
----------------------------------------------------------------------------------------------------
| epoch   1 step  1188200 |   4200 batches | lr 5.98e-08 | ms/batch 4802.49 | loss  3.68 | ppl    39.681
| epoch   1 step  1188400 |   4400 batches | lr 5.78e-08 | ms/batch 2034.11 | loss  3.69 | ppl    39.881
| epoch   1 step  1188600 |   4600 batches | lr 5.58e-08 | ms/batch 2038.39 | loss  3.69 | ppl    40.021
| epoch   1 step  1188800 |   4800 batches | lr 5.39e-08 | ms/batch 2033.97 | loss  3.68 | ppl    39.785
| epoch   1 step  1189000 |   5000 batches | lr 5.2e-08 | ms/batch 2034.40 | loss  3.68 | ppl    39.638
| epoch   1 step  1189200 |   5200 batches | lr 5.01e-08 | ms/batch 2046.59 | loss  3.68 | ppl    39.651
| epoch   1 step  1189400 |   5400 batches | lr 4.83e-08 | ms/batch 2032.86 | loss  3.68 | ppl    39.694
| epoch   1 step  1189600 |   5600 batches | lr 4.65e-08 | ms/batch 2033.55 | loss  3.68 | ppl    39.761
| epoch   1 step  1189800 |   5800 batches | lr 4.47e-08 | ms/batch 2016.74 | loss  3.68 | ppl    39.670
| epoch   1 step  1190000 |   6000 batches | lr 4.3e-08 | ms/batch 2016.46 | loss  3.69 | ppl    39.847
| epoch   1 step  1190200 |   6200 batches | lr 4.13e-08 | ms/batch 2018.59 | loss  3.68 | ppl    39.802
| epoch   1 step  1190400 |   6400 batches | lr 3.96e-08 | ms/batch 2016.05 | loss  3.68 | ppl    39.662
| epoch   1 step  1190600 |   6600 batches | lr 3.8e-08 | ms/batch 2016.43 | loss  3.69 | ppl    39.874
| epoch   1 step  1190800 |   6800 batches | lr 3.64e-08 | ms/batch 2016.40 | loss  3.68 | ppl    39.841
| epoch   1 step  1191000 |   7000 batches | lr 3.48e-08 | ms/batch 2016.25 | loss  3.68 | ppl    39.658
| epoch   1 step  1191200 |   7200 batches | lr 3.33e-08 | ms/batch 2016.36 | loss  3.68 | ppl    39.496
| epoch   1 step  1191400 |   7400 batches | lr 3.18e-08 | ms/batch 2025.33 | loss  3.68 | ppl    39.713
| epoch   1 step  1191600 |   7600 batches | lr 3.03e-08 | ms/batch 2017.72 | loss  3.69 | ppl    39.847
| epoch   1 step  1191800 |   7800 batches | lr 2.89e-08 | ms/batch 2017.02 | loss  3.68 | ppl    39.711
| epoch   1 step  1192000 |   8000 batches | lr 2.75e-08 | ms/batch 2015.76 | loss  3.68 | ppl    39.673
----------------------------------------------------------------------------------------------------
| Eval 298 at step  1192000 | time: 8636.21s | valid loss  3.72 | valid ppl    41.442
----------------------------------------------------------------------------------------------------
| epoch   1 step  1192200 |   8200 batches | lr 2.61e-08 | ms/batch 4696.97 | loss  3.68 | ppl    39.778
| epoch   1 step  1192400 |   8400 batches | lr 2.48e-08 | ms/batch 2014.29 | loss  3.69 | ppl    39.865
| epoch   1 step  1192600 |   8600 batches | lr 2.35e-08 | ms/batch 2014.38 | loss  3.68 | ppl    39.725
| epoch   1 step  1192800 |   8800 batches | lr 2.23e-08 | ms/batch 2014.19 | loss  3.68 | ppl    39.545
| epoch   1 step  1193000 |   9000 batches | lr 2.1e-08 | ms/batch 2013.96 | loss  3.68 | ppl    39.472
| epoch   1 step  1193200 |   9200 batches | lr 1.99e-08 | ms/batch 2015.06 | loss  3.68 | ppl    39.792
| epoch   1 step  1193400 |   9400 batches | lr 1.87e-08 | ms/batch 2014.12 | loss  3.69 | ppl    39.862
| epoch   1 step  1193600 |   9600 batches | lr 1.76e-08 | ms/batch 2013.87 | loss  3.68 | ppl    39.701
| epoch   1 step  1193800 |   9800 batches | lr 1.65e-08 | ms/batch 2014.95 | loss  3.68 | ppl    39.770
| epoch   1 step  1194000 |  10000 batches | lr 1.55e-08 | ms/batch 2019.76 | loss  3.68 | ppl    39.618
| epoch   1 step  1194200 |  10200 batches | lr 1.44e-08 | ms/batch 2013.09 | loss  3.68 | ppl    39.765
| epoch   1 step  1194400 |  10400 batches | lr 1.35e-08 | ms/batch 2012.83 | loss  3.68 | ppl    39.796
| epoch   1 step  1194600 |  10600 batches | lr 1.25e-08 | ms/batch 2012.88 | loss  3.68 | ppl    39.665
| epoch   1 step  1194800 |  10800 batches | lr 1.16e-08 | ms/batch 2013.06 | loss  3.68 | ppl    39.711
| epoch   1 step  1195000 |  11000 batches | lr 1.07e-08 | ms/batch 2014.02 | loss  3.69 | ppl    40.074
| epoch   1 step  1195200 |  11200 batches | lr 9.9e-09 | ms/batch 2012.72 | loss  3.68 | ppl    39.554
| epoch   1 step  1195400 |  11400 batches | lr 9.09e-09 | ms/batch 2012.64 | loss  3.69 | ppl    39.904
| epoch   1 step  1195600 |  11600 batches | lr 8.32e-09 | ms/batch 2012.27 | loss  3.68 | ppl    39.538
| epoch   1 step  1195800 |  11800 batches | lr 7.58e-09 | ms/batch 2013.45 | loss  3.68 | ppl    39.646
| epoch   1 step  1196000 |  12000 batches | lr 6.87e-09 | ms/batch 2019.77 | loss  3.68 | ppl    39.825
----------------------------------------------------------------------------------------------------
| Eval 299 at step  1196000 | time: 8585.98s | valid loss  3.72 | valid ppl    41.443
----------------------------------------------------------------------------------------------------
| epoch   1 step  1196200 |  12200 batches | lr 6.2e-09 | ms/batch 4654.69 | loss  3.68 | ppl    39.774
| epoch   1 step  1196400 |  12400 batches | lr 5.57e-09 | ms/batch 2011.27 | loss  3.68 | ppl    39.840
| epoch   1 step  1196600 |  12600 batches | lr 4.97e-09 | ms/batch 2011.16 | loss  3.68 | ppl    39.725
| epoch   1 step  1196800 |  12800 batches | lr 4.4e-09 | ms/batch 2009.67 | loss  3.69 | ppl    40.065
| epoch   1 step  1197000 |  13000 batches | lr 3.87e-09 | ms/batch 2008.45 | loss  3.68 | ppl    39.559
| epoch   1 step  1197200 |  13200 batches | lr 3.37e-09 | ms/batch 2009.02 | loss  3.68 | ppl    39.579
| epoch   1 step  1197400 |  13400 batches | lr 2.9e-09 | ms/batch 2008.52 | loss  3.68 | ppl    39.710
| epoch   1 step  1197600 |  13600 batches | lr 2.47e-09 | ms/batch 2009.35 | loss  3.69 | ppl    39.853
| epoch   1 step  1197800 |  13800 batches | lr 2.08e-09 | ms/batch 2012.22 | loss  3.68 | ppl    39.770
| epoch   1 step  1198000 |  14000 batches | lr 1.72e-09 | ms/batch 2009.75 | loss  3.69 | ppl    39.890
| epoch   1 step  1198200 |  14200 batches | lr 1.39e-09 | ms/batch 2009.50 | loss  3.68 | ppl    39.750
| epoch   1 step  1198400 |  14400 batches | lr 1.1e-09 | ms/batch 2009.45 | loss  3.69 | ppl    39.860
| epoch   1 step  1198600 |  14600 batches | lr 8.42e-10 | ms/batch 2010.71 | loss  3.68 | ppl    39.744
| epoch   1 step  1198800 |  14800 batches | lr 6.19e-10 | ms/batch 2009.92 | loss  3.68 | ppl    39.842
| epoch   1 step  1199000 |  15000 batches | lr 4.3e-10 | ms/batch 2008.93 | loss  3.68 | ppl    39.818
| epoch   2 step  1199200 |     85 batches | lr 2.75e-10 | ms/batch 2013.19 | loss  3.69 | ppl    39.878
| epoch   2 step  1199400 |    285 batches | lr 1.55e-10 | ms/batch 2008.66 | loss  3.68 | ppl    39.513
| epoch   2 step  1199600 |    485 batches | lr 6.87e-11 | ms/batch 2009.13 | loss  3.68 | ppl    39.531
| epoch   2 step  1199800 |    685 batches | lr 1.72e-11 | ms/batch 2008.78 | loss  3.69 | ppl    39.888
| epoch   2 step  1200000 |    885 batches | lr 0 | ms/batch 2008.31 | loss  3.68 | ppl    39.585
----------------------------------------------------------------------------------------------------
| Eval 300 at step  1200000 | time: 8553.18s | valid loss  3.72 | valid ppl    41.443
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
End of training
====================================================================================================
| End of training | test loss  4.20 | test ppl    66.393
====================================================================================================
